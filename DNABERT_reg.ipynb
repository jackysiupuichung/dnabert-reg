{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DNABERT_reg.ipynb","provenance":[],"machine_shape":"hm","authorship_tag":"ABX9TyPmD6fB56kvZmfemE64h7DL"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"XaL4dzTiV5cv","executionInfo":{"status":"ok","timestamp":1641388950272,"user_tz":0,"elapsed":1967,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"6608136b-e4d3-413c-c193-872969ba2a61"},"source":["# Load the Drive helper and mount\n","from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","metadata":{"id":"ZlGhaGHY1WV5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641388954859,"user_tz":0,"elapsed":4677,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"16eeb052-2395-4836-a509-f6917c5c52c9"},"source":["!pip install transformers"],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.15.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.4.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.2.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.6)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.1.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"wmQPCY8702co","executionInfo":{"status":"ok","timestamp":1641388954859,"user_tz":0,"elapsed":88,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["from transformers import BertForSequenceClassification, BertTokenizer, BertConfig,AdamW , get_linear_schedule_with_warmup\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import mean_squared_error\n","from sklearn.metrics import r2_score\n","import itertools\n","import pandas as pd\n","import numpy as np\n","import random"],"execution_count":40,"outputs":[]},{"cell_type":"code","source":["random.seed(42)"],"metadata":{"id":"PNSXyJT83If6","executionInfo":{"status":"ok","timestamp":1641388954860,"user_tz":0,"elapsed":87,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":41,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vhfCOUX0a9H1","executionInfo":{"status":"ok","timestamp":1641388954860,"user_tz":0,"elapsed":87,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"97ecb7ff-b72f-4c70-89df-11711185b1c4"},"source":["cd /content/drive/MyDrive/master_thesis/inputs/"],"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/master_thesis/inputs\n"]}]},{"cell_type":"code","source":["ls"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"44M-RlFZ2-yS","executionInfo":{"status":"ok","timestamp":1641388954861,"user_tz":0,"elapsed":87,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"3b157612-16b6-4fa1-d047-0f3e4a835638"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["ENCFF292FVY.tsv  ENCFF910TAZ.tsv  \u001b[0m\u001b[01;34mhg38_msxTm_ENCFF292FVY\u001b[0m/  hg38_msxTm.txt\n","ENCFF694PZC.tsv  \u001b[01;34mfind_motifs\u001b[0m/     \u001b[01;34mhg38_msxTm_ENCFF910TAZ\u001b[0m/\n"]}]},{"cell_type":"code","source":["input_csv = \"hg38_msxTm_ENCFF910TAZ/hg38_msxTm_ENCFF910TAZ.csv\""],"metadata":{"id":"HSl6gMSTva_d","executionInfo":{"status":"ok","timestamp":1641388954862,"user_tz":0,"elapsed":81,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":44,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UDl0v5ezarma"},"source":["Import input that was parsed using inputparser.ipynb"]},{"cell_type":"code","metadata":{"id":"H-xoqJnD426N","executionInfo":{"status":"ok","timestamp":1641388954863,"user_tz":0,"elapsed":82,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["final_matrix = pd.read_csv(input_csv)"],"execution_count":45,"outputs":[]},{"cell_type":"markdown","source":["choose pretrained model from 3 to 6 mer (!!!change seq2kmer k)"],"metadata":{"id":"Yjffz4Jp_TAy"}},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"drh-uppF0avG","executionInfo":{"status":"ok","timestamp":1641388954863,"user_tz":0,"elapsed":82,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"a749cecc-446d-4262-d587-16ef82c5cbe2"},"source":["cd /content/drive/MyDrive/master_thesis/models/4-new-12w-0/"],"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/master_thesis/models/4-new-12w-0\n"]}]},{"cell_type":"code","metadata":{"id":"1izCagm9gYOL","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641388954864,"user_tz":0,"elapsed":80,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"b91ed0e3-3941-427f-b366-cd19c129633d"},"source":["import torch\n","if torch.cuda.is_available():       \n","    device = torch.device(\"cuda\")\n","    print(\"Using GPU.\")\n","else:\n","    print(\"No GPU available, using the CPU instead.\")\n","    device = torch.device(\"cpu\")"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Using GPU.\n"]}]},{"cell_type":"markdown","metadata":{"id":"5eVuxQiSH8RY"},"source":["Parse sample data into pytorch object to pass through BERT trainer https://huggingface.co/transformers/custom_datasets.html https://huggingface.co/transformers/main_classes/trainer.html#transformers.Trainer\n","https://huggingface.co/transformers/custom_datasets.html"]},{"cell_type":"code","metadata":{"id":"3VU2UVJTL-Fp","executionInfo":{"status":"ok","timestamp":1641386396151,"user_tz":0,"elapsed":811,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["DNABERT_Tokenizer = BertTokenizer(vocab_file = \"vocab.txt\", config = \"tokenizer_config.json\")"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIITxOb9qvtB","executionInfo":{"status":"ok","timestamp":1641386396152,"user_tz":0,"elapsed":85,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["def seq2kmer(seq, k = 4):\n","    \"\"\"\n","    Function provided by Ji et al. (https://github.com/jerryji1993/DNABERT)\n","    Will convert given sequence into kmer.\n","    \"\"\"\n","    kmer = [seq[x:x+k] for x in range(len(seq)+1-k)]\n","    kmers = \" \".join(kmer)\n","    return kmer"],"execution_count":12,"outputs":[]},{"cell_type":"code","metadata":{"id":"3y2EsgVi1El8","executionInfo":{"status":"ok","timestamp":1641386396153,"user_tz":0,"elapsed":84,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["def tokenize(seq_kmers, labels):\n","  input_ids = []\n","  attention_masks = []\n","\n","  for sent in seq_kmers:\n","      encoded_dict = DNABERT_Tokenizer.encode_plus(\n","                          sent,                      # Sentence to encode.\n","                          add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n","                          # Pad & truncate all sentences. (does not apply for our data since the lengths are all the same)\n","                          max_length = 297,\n","                          pad_to_max_length = True,\n","                          return_attention_mask = True,   # Construct attention masks.\n","                          return_tensors = 'pt',     # Return pytorch tensors.\n","                    )\n","      \n","      # Add the encoded sentence to the list.    \n","      input_ids.append(encoded_dict['input_ids'])\n","      \n","      # And its attention mask (simply differentiates padding from non-padding).\n","      attention_masks.append(encoded_dict['attention_mask'])\n","\n","  input_ids = torch.cat(input_ids, dim=0)\n","  attention_masks = torch.cat(attention_masks, dim=0)\n","  labels = torch.tensor(labels)\n","\n","  # Combine the training inputs into a TensorDataset.\n","  dataset = TensorDataset(input_ids, attention_masks, labels)\n","  return dataset"],"execution_count":13,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBq3iQHGBn-w","executionInfo":{"status":"ok","timestamp":1641386397222,"user_tz":0,"elapsed":1152,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"0757f8ea-bb6a-4c27-8d99-4f633372eec7"},"source":["seq_kmers = list(map(seq2kmer, final_matrix[\"Sequence\"].tolist()))\n","dataset = tokenize(seq_kmers, final_matrix[\"logTPM\"].tolist())"],"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.7/dist-packages/transformers/tokenization_utils_base.py:2232: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n"]}]},{"cell_type":"code","metadata":{"id":"uKFhAkyz2mch","executionInfo":{"status":"ok","timestamp":1641386417269,"user_tz":0,"elapsed":228,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["train_dataset, test_dataset = train_test_split(dataset, test_size=0.2, random_state=1)\n","train_dataset, val_dataset = train_test_split(train_dataset, test_size=0.25, random_state=1)"],"execution_count":16,"outputs":[]},{"cell_type":"code","source":["print(\"dataset size = \", len(dataset))\n","print(\"train_dataset size = \", len(train_dataset))\n","print(\"val_dataset size = \", len(val_dataset))\n","print(\"test_dataset size = \", len(test_dataset))\n"],"metadata":{"id":"7peFnYpXQZme","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641386426504,"user_tz":0,"elapsed":352,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"d3fe71ed-1616-4ee8-ea38-94d3ed65c0d6"},"execution_count":17,"outputs":[{"output_type":"stream","name":"stdout","text":["dataset size =  1919\n","train_dataset size =  1151\n","val_dataset size =  384\n","test_dataset size =  384\n"]}]},{"cell_type":"code","metadata":{"id":"ZcKtJz882Da-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641386462961,"user_tz":0,"elapsed":6734,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"eba67fed-b484-4444-f84b-eeb0f2a685e0"},"source":["model_config_path = \"./config.json\"\n","model_config =BertConfig.from_pretrained(model_config_path, num_labels = 1)\n","model_weight_path = \"./pytorch_model.bin\"\n","dnabert = BertForSequenceClassification.from_pretrained(model_weight_path, config = model_config)"],"execution_count":19,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at ./pytorch_model.bin were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./pytorch_model.bin and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]}]},{"cell_type":"code","metadata":{"id":"52e5EUO3XvvX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641386466191,"user_tz":0,"elapsed":382,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"994191cb-8533-415a-e44b-923fa91024aa"},"source":["# Get all of the model's parameters as a list of tuples.\n","params = list(dnabert.named_parameters())\n","\n","print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n","\n","print('==== Embedding Layer ====\\n')\n","\n","for p in params[0:5]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== First Transformer ====\\n')\n","\n","for p in params[5:21]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n","\n","print('\\n==== Output Layer ====\\n')\n","\n","for p in params[-4:]:\n","    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["The BERT model has 201 different named parameters.\n","\n","==== Embedding Layer ====\n","\n","bert.embeddings.word_embeddings.weight                    (261, 768)\n","bert.embeddings.position_embeddings.weight                (512, 768)\n","bert.embeddings.token_type_embeddings.weight                (2, 768)\n","bert.embeddings.LayerNorm.weight                              (768,)\n","bert.embeddings.LayerNorm.bias                                (768,)\n","\n","==== First Transformer ====\n","\n","bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.query.bias                (768,)\n","bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n","bert.encoder.layer.0.attention.self.key.bias                  (768,)\n","bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n","bert.encoder.layer.0.attention.self.value.bias                (768,)\n","bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n","bert.encoder.layer.0.attention.output.dense.bias              (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n","bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n","bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n","bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n","bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n","bert.encoder.layer.0.output.dense.bias                        (768,)\n","bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n","bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n","\n","==== Output Layer ====\n","\n","bert.pooler.dense.weight                                  (768, 768)\n","bert.pooler.dense.bias                                        (768,)\n","classifier.weight                                           (1, 768)\n","classifier.bias                                                 (1,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"ni6lsti7F5lV"},"source":["BERT authors suggest:\n","batch sizes: 8, 16, 32, 64, 128\n","learning rates: 3e-4, 1e-4, 5e-5, 3e-5\n","\n","https://github.com/google-research/bert"]},{"cell_type":"code","metadata":{"id":"_iMYAV0Rwy8x"},"source":["batch_size = 16\n","\n","train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","validation_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n","\n","batch_lr = {8:3e-4, 16:1e-4, 32: 5e-5, 64: 3e-5, 128: 3e-5}\n","\n","optimizer = AdamW(dnabert.parameters(), lr=batch_lr[batch_size])\n","\n","# Number of training epochs. The BERT authors recommend between 2 and 4. \n","# We chose to run for 4, but we'll see later that this may be over-fitting the\n","# training data.\n","epochs = 4\n","\n","# Total number of training steps is [number of batches] x [number of epochs]. \n","# (Note that this is not the same as the number of training samples).\n","total_steps = len(train_dataloader) * epochs\n","\n","# Create the learning rate scheduler.\n","scheduler = get_linear_schedule_with_warmup(optimizer, \n","                                            num_warmup_steps = 0, # Default value in run_glue.py\n","                                            num_training_steps = total_steps)\n","\n","model = dnabert\n","model.to(device)                                  "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H-wSGAcE8xHr"},"source":["def train(model, optimizer, scheduler, epochs,       \n","          train_dataloader, validation_dataloader, device, clip_value=2):\n","    training_stats = []\n","\n","    for epoch_i in range(0, epochs):\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","        total_train_loss = 0\n","        for batch in train_dataloader:\n","            input_ids = batch[0].to(device)\n","            attention_mask = batch[1].to(device)\n","            labels = batch[2].to(device)\n","            #https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch\n","            \n","            optimizer.zero_grad()\n","            model.zero_grad()\n","\n","            outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n","            loss = outputs[0]\n","            total_train_loss += loss.item()\n","            loss.backward()\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), clip_value)\n","            optimizer.step()\n","            scheduler.step()\n","\n","        avg_train_loss = total_train_loss / len(train_dataloader)\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","        \n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():\n","                output = model(b_input_ids, \n","                                      token_type_ids=None, \n","                                      attention_mask=b_input_mask,\n","                                      labels=b_labels)\n","            \n","            loss = output[0]\n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","            }\n","        )\n","    return model, training_stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fY0_L9G7gsWE"},"source":["ft_dnabert_model, training_stats = train(model, optimizer, scheduler, epochs, \n","              train_dataloader, validation_dataloader, device, clip_value=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gkfq8gEuqG1s"},"source":["import pandas as pd\n","\n","# Display floats with two decimal places.\n","pd.set_option('precision', 2)\n","\n","# Create a DataFrame from our training statistics.\n","df_stats = pd.DataFrame(data=training_stats)\n","\n","# Use the 'epoch' as the row index.\n","df_stats = df_stats.set_index('epoch')\n","\n","# Display the table.\n","df_stats"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GP2TQIgN82gt"},"source":["import matplotlib.pyplot as plt\n","% matplotlib inline\n","\n","import seaborn as sns\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks(list(range(1, 5)))\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zw_Bx_YE0BJC","executionInfo":{"status":"ok","timestamp":1641386483603,"user_tz":0,"elapsed":113,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"66c56425-3433-4a70-cf66-86cc5eb14657"},"source":["cd /content/drive/MyDrive/master_thesis/models/4-new-12w-0/ft_dnabert_model_4mer_save/"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/master_thesis/models/4-new-12w-0/ft_dnabert_model_4mer_save\n"]}]},{"cell_type":"code","metadata":{"id":"foMgBPc41OT2","executionInfo":{"status":"ok","timestamp":1641386491333,"user_tz":0,"elapsed":6416,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["DNABERT_Tokenizer = BertTokenizer(vocab_file = \"vocab.txt\", config = \"tokenizer_config.json\")\n","\n","ft_model_config_path = \"./config.json\"\n","ft_model_config =BertConfig.from_pretrained(ft_model_config_path, num_labels = 1)\n","\n","ft_model_weight_path = \"./pytorch_model.bin\"\n","ft_dnabert_model = BertForSequenceClassification.from_pretrained(ft_model_weight_path, config = ft_model_config)"],"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":["Save the best performing model (4mer) in output dir"],"metadata":{"id":"D2Cnyt3asgmu"}},{"cell_type":"code","metadata":{"id":"vQRTxuNMrNL2"},"source":["import os\n","\n","# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n","\n","output_dir = './ft_dnabert_model_4mer_save/'\n","\n","# Create output directory if needed\n","if not os.path.exists(output_dir):\n","    os.makedirs(output_dir)\n","\n","print(\"Saving model to %s\" % output_dir)\n","\n","# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n","# They can then be reloaded using `from_pretrained()`\n","model_to_save = model.module if hasattr(ft_dnabert_model, 'module') else ft_dnabert_model  # Take care of distributed/parallel training\n","model_to_save.save_pretrained(output_dir)\n","DNABERT_Tokenizer.save_pretrained(output_dir)\n","\n","# Good practice: save your training arguments together with the trained model\n","#torch.save(args, os.path.join(output_dir, 'training_args.bin'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wC0H0tUD2rKR"},"source":["Make prediction on test data"]},{"cell_type":"code","metadata":{"id":"Ax8b7hS_3hY2","executionInfo":{"status":"ok","timestamp":1641386492149,"user_tz":0,"elapsed":6,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"source":["def predict(model, test_dataset, device):\n","\n","  batch_size = 1\n","  prediction_dataloader = DataLoader(test_dataset, batch_size, shuffle=True)\n","  # Put model in evaluation mode\n","  model.eval()\n","  # Tracking variables\n","  max_seq_length = 297 #this variable is subject to change when kmer model and sequence length varies\n","  preds = None\n","  true_labels = []\n","  attention_scores = np.zeros([len(test_dataset), 12, max_seq_length, max_seq_length])\n","  sequences = []\n","  model.to(device)\n","  # ========================================\n","  #               Prediction\n","  # ========================================\n","  print(\"\")\n","  print(\"Running Prediction...\")\n","\n","  for index, batch in enumerate(prediction_dataloader):\n","      b_input_ids = batch[0].to(device)\n","      b_input_mask = batch[1].to(device)\n","      b_labels = batch[2].to(device)\n","\n","      # Telling the model not to compute or store gradients, saving memory and \n","      # speeding up prediction\n","      with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","          #b_label not required for predictions, no loss is calucalated\n","          outputs = model(b_input_ids, token_type_ids=None, \n","                          attention_mask=b_input_mask, output_attentions=True)\n","          \"\"\"\n","          attention = outputs[-1][-1]\n","          _, logits = outputs[:2]\n","\n","          preds[index*batch_size:index*batch_size+len(batch[0]),:] = logits.detach().cpu().numpy()\n","          attention_scores[index*batch_size:index*batch_size+len(batch[0]),:,:,:] = attention.cpu().numpy()\n","                # if preds is None:\n","                #     preds = logits.detach().cpu().numpy()\n","                # else:\n","                #     preds = np.concatenate((preds, logits.detach().cpu().numpy()), axis=0)\n","\n","                # if attention_scores is not None:\n","                #     attention_scores = np.concatenate((attention_scores, attention.cpu().numpy()), 0)\n","                # else:\n","                #     attention_scores = attention.cpu().numpy()\n","          \"\"\"\n","          logits = outputs[0]\n","          if preds is None:\n","              preds = logits.detach().cpu().numpy()\n","          else:\n","              preds = np.concatenate((preds, logits.detach().cpu().numpy()), axis=0)\n","          true_labels.append(b_labels)\n","          attention = outputs[-1][-1].cpu().numpy()\n","          attention_scores[index*batch_size:index*batch_size+len(batch[0]),:,:,:] = attention\n","          sequences.append(b_input_ids.cpu().tolist())\n","  \n","  sequences=np.array(sequences).squeeze()\n","\n","\n","  print(\"\")\n","  print(\"Prediction completed\")\n","\n","  return preds, true_labels, attention_scores, sequences\n"],"execution_count":24,"outputs":[]},{"cell_type":"code","source":["torch.cuda.empty_cache()"],"metadata":{"id":"EtzUU8_DFBaH","executionInfo":{"status":"ok","timestamp":1641386495778,"user_tz":0,"elapsed":236,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","metadata":{"id":"FILe8CvZhv4J","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1641329902313,"user_tz":0,"elapsed":19840,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"d89afb1f-f35e-4085-facd-bb4a5181b422"},"source":["predictions, labels, attention_scores, sequences = predict(ft_dnabert_model, test_dataset, device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Running Prediction...\n","\n","Prediction completed\n"]}]},{"cell_type":"code","source":["cd /content/drive/My Drive/master_thesis/inputs/find_motifs/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xYao2aLly0_r","executionInfo":{"status":"ok","timestamp":1641389208967,"user_tz":0,"elapsed":309,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"0d358a25-5acd-479f-9849-e1929fc3a8ba"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/My Drive/master_thesis/inputs/find_motifs\n"]}]},{"cell_type":"code","source":["#removed probs because dont know what it does yet\n","\n","def unknown_codes(attention_scores, kmer = 4):\n","\n","    \"\"\"\n","    Seems like it reduced dimension from (batchsize, heads, kmer-len, kmer-len) \n","    to (batchsize, kmer-len)\n","    \"\"\"\n","    scores = np.zeros([attention_scores.shape[0], attention_scores.shape[-1]])\n","\n","    for index, attention_score in enumerate(attention_scores):\n","        attn_score = []\n","        for i in range(1, attention_score.shape[-1]-kmer+2):\n","            attn_score.append(float(attention_score[:,0,i].sum()))\n","\n","        for i in range(len(attn_score)-1):\n","            if attn_score[i+1] == 0:\n","                attn_score[i] = 0\n","                break\n","\n","        # attn_score[0] = 0    \n","        counts = np.zeros([len(attn_score)+kmer-1])\n","        real_scores = np.zeros([len(attn_score)+kmer-1])\n","        for i, score in enumerate(attn_score):\n","            for j in range(kmer):\n","                counts[i+j] += 1.0\n","                real_scores[i+j] += score\n","        real_scores = real_scores / counts\n","        real_scores = real_scores / np.linalg.norm(real_scores)\n","\n","        scores[index] = real_scores\n","        \n","\n","    return scores"],"metadata":{"id":"BIjQnjxeRDXp","executionInfo":{"status":"ok","timestamp":1641387010449,"user_tz":0,"elapsed":290,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":34,"outputs":[]},{"cell_type":"code","source":["def kmer2seq(kmers):\n","    \"\"\"\n","    Convert kmers to original sequence\n","    \n","    Arguments:\n","    kmers -- str, kmers separated by space.\n","    \n","    Returns:\n","    seq -- str, original sequence.\n","    \"\"\"\n","    kmers_list = kmers.split(\" \")\n","    bases = [kmer[0] for kmer in kmers_list[0:-1]]\n","    bases.append(kmers_list[-1])\n","    seq = \"\".join(bases)\n","    assert len(seq) == len(kmers_list) + len(kmers_list[0]) - 1\n","    return seq"],"metadata":{"id":"p1NaV2OZcJD6","executionInfo":{"status":"ok","timestamp":1641389529550,"user_tz":0,"elapsed":309,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["def decode(tokenized_seqs):\n","    seqs = None\n","    for tokenized_seq in tokenized_seqs:\n","        seq = DNABERT_Tokenizer.decode(tokenized_seq,\n","                                        skip_special_tokens = True\n","                                        )\n","        seq = kmer2seq(seq)\n","        \n","        if seqs is None:\n","            seqs = seq\n","        else:\n","            seqs = np.hstack((seqs, seq))\n","    return seqs"],"metadata":{"id":"UC_1ojx6Zod_","executionInfo":{"status":"ok","timestamp":1641389544805,"user_tz":0,"elapsed":308,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["sequences = np.load(\"sequences.npy\")"],"metadata":{"id":"exLrhalXa28T","executionInfo":{"status":"ok","timestamp":1641389211923,"user_tz":0,"elapsed":628,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":53,"outputs":[]},{"cell_type":"code","source":["sequences_2 = decode(sequences)"],"metadata":{"id":"HYj7_gmjayIs","executionInfo":{"status":"ok","timestamp":1641389551956,"user_tz":0,"elapsed":3055,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":67,"outputs":[]},{"cell_type":"code","source":["print(sequences_2[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u_-ky818b63v","executionInfo":{"status":"ok","timestamp":1641389555894,"user_tz":0,"elapsed":386,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}},"outputId":"1320dc1d-5bd3-4ad4-a356-2b21ea40f488"},"execution_count":68,"outputs":[{"output_type":"stream","name":"stdout","text":["GGAGGAGTTGCACAGTCAATGCCTTGTTCAAGATTAATCATTTTCCCTGTTGCCTATGACTGAGGCTGTTTCTCACATCCTGCCCCTGGTTTTGCCTGTCCCAAGGATTTCGTCTGAAGGGCGGGACATTCCCCCTGCCTCTTCGCACCACAGCCAGAGCCTGCCATTAGGACCAATGAAAGCAAAGTACCTCATCCCCTCAGTGACTAAGAATCGCAGTATTTAAGAGGTAGCAGGAATGGGCTGAGAGTGGTGTTTGCTTTCTCCACCAGAAGGGCACACTTTCATCTAATTTGGG\n"]}]},{"cell_type":"code","source":["att_2 = unknown_codes(att)\n","np.save(\"positives_seq.npy\", sequences_2) #save positive sequences in seq format\n","np.save(\"attention_scores_2d.npy\", att_2) #save attention_scores for positive sequences in (batchsize, no.kmer)"],"metadata":{"id":"9kk0Ce8JSYBk","executionInfo":{"status":"ok","timestamp":1641389577673,"user_tz":0,"elapsed":295,"user":{"displayName":"Jacky Siu","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18114207530306187606"}}},"execution_count":69,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-79xOnCS53MF"},"source":["Interpret prediction vs true label\n","\n","https://stats.stackexchange.com/questions/131267/how-to-interpret-error-measures"]},{"cell_type":"code","metadata":{"id":"2XFXTBrQ7faB"},"source":["def unlist(predictions, true_labels):\n","    predictions = (list(itertools.chain(*predictions)))\n","    predictions = (list(itertools.chain(*predictions)))\n","    true_labels = [label.tolist() for label in true_labels]\n","    true_labels = (list(itertools.chain(*true_labels)))\n","    return predictions, true_labels\n","\n","def measurement_metric(predictions, true_labels, measurement):\n","    metric = measurement(true_labels, predictions)               \n","    return metric"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i5FV6K7F-sUj"},"source":["pred, lab = unlist(predictions, labels)\n","mse = measurement_metric(pred, lab, mean_squared_error)\n","r2 = measurement_metric(pred, lab, r2_score)\n","print(mse, r2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["plt.figure(figsize=(10,10))\n","plt.scatter(lab, pred, c='crimson')\n","#plt.yscale('log')\n","#plt.xscale('log')\n","\n","p1 = max(max(pred), max(lab))\n","p2 = min(min(pred), min(lab))\n","plt.plot([p1, p2], [p1, p2], 'b-')\n","plt.xlabel('True Values', fontsize=15)\n","plt.ylabel('Predictions', fontsize=15)\n","plt.axis('equal')\n","plt.show()"],"metadata":{"id":"gc9RjbR94vHr"},"execution_count":null,"outputs":[]}]}